{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "mnist = keras.datasets.mnist\n",
    "(mnist_train_x, mnist_train_y), (mnist_test_x, mnist_test_y)\\\n",
    "    = mnist.load_data()\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage import exposure\n",
    "B= []\n",
    "for i in range(len(mnist_train_x)):\n",
    "    A = mnist_train_x[i]\n",
    "    A = exposure.rescale_intensity(A, out_range=(0, 255))\n",
    "    A = imutils.resize(A, width=32)\n",
    "    B.append(A)\n",
    "B = np.array(B)\n",
    "\n",
    "mnist_train_RGB_x = np.repeat(B[:,:, :, np.newaxis], 3, axis=3)\n",
    "B= []\n",
    "for i in range(len(mnist_test_x)):\n",
    "    A = mnist_test_x[i]\n",
    "    A = exposure.rescale_intensity(A, out_range=(0, 255))\n",
    "    A = imutils.resize(A, width=32)\n",
    "    B.append(A)\n",
    "B = np.array(B)\n",
    "\n",
    "mnist_test_RGB_x = np.repeat(B[:,:, :, np.newaxis], 3, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_train_y = np.array([[mnist_train_y[i]] for i in range(len(mnist_train_y))])\n",
    "M_test_y = np.array([[mnist_test_y[i]] for i in range(len(mnist_test_y))])\n",
    "(C_x_train, C_y_train), (C_x_test, C_y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1001 19:46:33.125200 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1001 19:46:33.152634 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1001 19:46:33.158618 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1001 19:46:33.186544 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1001 19:46:33.186544 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1001 19:46:35.201187 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1001 19:46:35.257041 25180 deprecation.py:506] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1001 19:46:35.326820 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1001 19:46:39.210970 25180 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1001 19:46:39.425957 25180 deprecation.py:323] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " - 50s - loss: 20.0786 - acc: 0.5210 - val_loss: 21.2302 - val_acc: 0.1212\n",
      "Epoch 2/25\n",
      " - 44s - loss: 8.4775 - acc: 0.8307 - val_loss: 8.0610 - val_acc: 0.3570\n",
      "Epoch 3/25\n",
      " - 43s - loss: 3.6573 - acc: 0.9149 - val_loss: 5.3086 - val_acc: 0.3078\n",
      "Epoch 4/25\n",
      " - 44s - loss: 1.7818 - acc: 0.9394 - val_loss: 1.5397 - val_acc: 0.8567\n",
      "Epoch 5/25\n",
      " - 43s - loss: 1.0077 - acc: 0.9495 - val_loss: 1.2960 - val_acc: 0.7996\n",
      "Epoch 6/25\n",
      " - 43s - loss: 0.7414 - acc: 0.9516 - val_loss: 1.6076 - val_acc: 0.8033\n",
      "Epoch 7/25\n",
      " - 43s - loss: 0.6663 - acc: 0.9553 - val_loss: 0.5313 - val_acc: 0.9734\n",
      "Epoch 8/25\n",
      " - 43s - loss: 0.5612 - acc: 0.9574 - val_loss: 0.5151 - val_acc: 0.9616\n",
      "Epoch 9/25\n",
      " - 43s - loss: 0.5314 - acc: 0.9593 - val_loss: 0.4770 - val_acc: 0.9696\n",
      "Epoch 10/25\n",
      " - 43s - loss: 0.5040 - acc: 0.9610 - val_loss: 0.5529 - val_acc: 0.9469\n",
      "Epoch 11/25\n",
      " - 44s - loss: 0.4950 - acc: 0.9615 - val_loss: 0.4368 - val_acc: 0.9710\n",
      "Epoch 12/25\n",
      " - 43s - loss: 0.4853 - acc: 0.9621 - val_loss: 0.4181 - val_acc: 0.9792\n",
      "Epoch 13/25\n",
      " - 43s - loss: 0.4836 - acc: 0.9628 - val_loss: 0.4759 - val_acc: 0.9642\n",
      "Epoch 14/25\n",
      " - 43s - loss: 0.4798 - acc: 0.9631 - val_loss: 0.4234 - val_acc: 0.9732\n",
      "Epoch 15/25\n",
      " - 43s - loss: 0.4742 - acc: 0.9625 - val_loss: 0.6248 - val_acc: 0.9251\n",
      "Epoch 16/25\n",
      " - 43s - loss: 0.4699 - acc: 0.9627 - val_loss: 0.4171 - val_acc: 0.9783\n",
      "Epoch 17/25\n",
      " - 43s - loss: 0.4750 - acc: 0.9642 - val_loss: 0.3951 - val_acc: 0.9804\n",
      "Epoch 18/25\n",
      " - 43s - loss: 0.4681 - acc: 0.9634 - val_loss: 0.4165 - val_acc: 0.9765\n",
      "Epoch 19/25\n",
      " - 43s - loss: 0.4638 - acc: 0.9645 - val_loss: 0.4223 - val_acc: 0.9745\n",
      "Epoch 20/25\n",
      " - 43s - loss: 0.4613 - acc: 0.9644 - val_loss: 0.4411 - val_acc: 0.9668\n",
      "Epoch 21/25\n",
      " - 43s - loss: 0.3876 - acc: 0.9741 - val_loss: 0.3149 - val_acc: 0.9850\n",
      "Epoch 22/25\n",
      " - 43s - loss: 0.3302 - acc: 0.9749 - val_loss: 0.2945 - val_acc: 0.9791\n",
      "Epoch 23/25\n",
      " - 43s - loss: 0.3075 - acc: 0.9740 - val_loss: 0.2570 - val_acc: 0.9860\n",
      "Epoch 24/25\n",
      " - 43s - loss: 0.3053 - acc: 0.9723 - val_loss: 0.2539 - val_acc: 0.9871\n",
      "Epoch 25/25\n",
      " - 43s - loss: 0.2964 - acc: 0.9746 - val_loss: 0.2664 - val_acc: 0.9821\n",
      "the validation 0/1 loss is:  0.4137\n"
     ]
    }
   ],
   "source": [
    "class MNIST_vgg:\n",
    "    def __init__(self,train=True):\n",
    "        self.num_classes = 10\n",
    "        self.weight_decay = 0.0005\n",
    "        self.x_shape = [32,32,3]\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.model = self.train(self.model)\n",
    "        else:\n",
    "            self.model.load_weights('MNIST_vgg.h5')\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
    "\n",
    "        model = Sequential()\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def normalize(self,X_train,X_test):\n",
    "        #this function normalize inputs for zero mean and unit variance\n",
    "        # it is used when training a model.\n",
    "        # Input: training set and test set\n",
    "        # Output: normalized training set and test set according to the trianing set statistics.\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        return X_train, X_test\n",
    "\n",
    "    def normalize_production(self,x):\n",
    "        #this function is used to normalize instances in production according to saved training set statistics\n",
    "        # Input: X - a training set\n",
    "        # Output X - a normalized training set according to normalization constants.\n",
    "\n",
    "        #these values produced during first training and are general for the standard cifar10 training set normalization\n",
    "        mean = 120.707\n",
    "        std = 64.15\n",
    "        return (x-mean)/(std+1e-7)\n",
    "\n",
    "    def predict(self,x,normalize=True,batch_size=50):\n",
    "        if normalize:\n",
    "            x = self.normalize_production(x)\n",
    "        return self.model.predict(x,batch_size)\n",
    "\n",
    "    def train(self,model):\n",
    "\n",
    "        #training parameters\n",
    "        batch_size = 128\n",
    "        maxepoches = 25\n",
    "        learning_rate = 0.1\n",
    "        lr_decay = 1e-6\n",
    "        lr_drop = 20\n",
    "        # The data, shuffled and split between train and test sets:\n",
    "        x_train,y_train,x_test,y_test = mnist_train_RGB_x,M_train_y,mnist_test_RGB_x,M_test_y\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train, x_test = self.normalize(x_train, x_test)\n",
    "\n",
    "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
    "\n",
    "        def lr_scheduler(epoch):\n",
    "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "        #data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "        #optimization details\n",
    "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # training process in a for loop with learning rate drop every 25 epoches.\n",
    "\n",
    "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                         batch_size=batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            epochs=maxepoches,\n",
    "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=2)\n",
    "        model.save_weights('MNIST_vgg.h5')\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    x_train,y_train,x_test,y_test = mnist_train_RGB_x,M_train_y,mnist_test_RGB_x,M_test_y\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    model = MNIST_vgg()\n",
    "\n",
    "    predicted_x = model.predict(x_test)\n",
    "    residuals = np.argmax(predicted_x,1)!=np.argmax(y_test,1)\n",
    "\n",
    "    loss = sum(residuals)/len(residuals)\n",
    "    print(\"the validation 0/1 loss is: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_submax(arr):\n",
    "    arr = np.array(arr)\n",
    "    MAX = np.max(arr)\n",
    "    idx = find_idx(arr, MAX)\n",
    "    arr_without_max = np.delete(arr,idx)\n",
    "    return np.max(arr_without_max)\n",
    "def find_statistics(Prob_Mat):\n",
    "    Prob_diff = []\n",
    "    MAX_Prob_Mat = []\n",
    "    MAX_Prob_Mat_idx = []\n",
    "    subMAX_Prob_Mat = []\n",
    "    subMAX_Prob_Mat_idx = []\n",
    "    for i in range(len(Prob_Mat)):\n",
    "        MAX = np.max(Prob_Mat[i])\n",
    "        MAX_idx = find_idx(Prob_Mat[i], MAX)[0]\n",
    "        subMAX = get_submax(Prob_Mat[i])\n",
    "        subMAX_idx = find_idx(Prob_Mat[i], subMAX)[0]\n",
    "        prob_difference = MAX - subMAX\n",
    "        Prob_diff.append(prob_difference)\n",
    "        MAX_Prob_Mat.append(MAX)\n",
    "        subMAX_Prob_Mat.append(subMAX)\n",
    "        MAX_Prob_Mat_idx.append(MAX_idx)\n",
    "        subMAX_Prob_Mat_idx.append(subMAX_idx)\n",
    "    return Prob_diff,MAX_Prob_Mat,MAX_Prob_Mat_idx,subMAX_Prob_Mat,subMAX_Prob_Mat_idx\n",
    "import pandas as pd\n",
    "def entropy_from_distribution(p, axis):\n",
    "    return np.log(10.) + np.sum(p * np.log(np.abs(p) + 1e-11), axis=1, keepdims=True)\n",
    "def conclusion_df(statistics, entropy):\n",
    "    d = {'Prob_diff': statistics[0], 'MAX_Prob_Mat': statistics[1], 'MAX_Prob_Mat_idx': statistics[2], 'subMAX_Prob_Mat': statistics[3], 'subMAX_Prob_Mat_idx': statistics[4], 'entropy':entropy}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n",
    "def plot_avg_dist(Prob_Mat):\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.hist(Prob_Mat[:,i], bins=20)\n",
    "        plt.title('Prob_dist' + str(i))\n",
    "    plt.show()\n",
    "    return\n",
    "def find_idx(arr, target):\n",
    "    ans = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] == target:\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_ID_OD(predicted_ID, predicted_OD):\n",
    "\n",
    "    statistics_ID = find_statistics(predicted_ID)\n",
    "    statistics_OD = find_statistics(predicted_OD)\n",
    "    entropy_ID = entropy_from_distribution(predicted_ID, axis=1)\n",
    "    entropy_ID = entropy_from_distribution(predicted_ID, axis=1).reshape(entropy_ID.shape[0])\n",
    "\n",
    "    entropy_OD = entropy_from_distribution(predicted_OD, axis=1)\n",
    "    entropy_OD = entropy_from_distribution(predicted_OD, axis=1).reshape(entropy_OD.shape[0])\n",
    "    df_ID_ID, df_ID_OD = conclusion_df(statistics_ID, entropy_ID), conclusion_df(statistics_OD, entropy_OD)\n",
    "\n",
    "    plot_avg_dist(predicted_ID)\n",
    "    plot_avg_dist(predicted_OD)\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(df_ID_ID['Prob_diff'], bins=20)\n",
    "    plt.title('ID_ID_prob_diff')\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(df_ID_ID['MAX_Prob_Mat_idx'], bins=20)\n",
    "    plt.title('ID_ID_max')\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(df_ID_ID['entropy'], bins=20)\n",
    "    plt.title('ID_ID_entropy')\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.hist(df_ID_OD['Prob_diff'], bins=20)\n",
    "    plt.title('ID_OD_prob_diff')\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(df_ID_OD['MAX_Prob_Mat_idx'], bins=20)\n",
    "    plt.title('ID_OD_max')\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.hist(df_ID_OD['entropy'], bins=20)\n",
    "    plt.title('ID_OD_entropy')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
