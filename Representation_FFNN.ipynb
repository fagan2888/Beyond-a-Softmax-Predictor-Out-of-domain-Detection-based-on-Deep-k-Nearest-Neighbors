{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple model. execution starts from the bottom\n",
    "def basic_mnist_model():\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx(arr, target):\n",
    "    ans = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] == target:\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(mnist_train_x, mnist_train_y), (mnist_test_x, mnist_test_y)\\\n",
    "    = mnist.load_data()\n",
    "def get_submax(arr):\n",
    "    arr = np.array(arr)\n",
    "    MAX = np.max(arr)\n",
    "    idx = find_idx(arr, MAX)\n",
    "    arr_without_max = np.delete(arr,idx)\n",
    "    return np.max(arr_without_max)\n",
    "def find_statistics(Prob_Mat):\n",
    "    Prob_diff = []\n",
    "    MAX_Prob_Mat = []\n",
    "    MAX_Prob_Mat_idx = []\n",
    "    subMAX_Prob_Mat = []\n",
    "    subMAX_Prob_Mat_idx = []\n",
    "    for i in range(len(Prob_Mat)):\n",
    "        MAX = np.max(Prob_Mat[i])\n",
    "        MAX_idx = find_idx(Prob_Mat[i], MAX)[0]\n",
    "        subMAX = get_submax(Prob_Mat[i])\n",
    "        subMAX_idx = find_idx(Prob_Mat[i], subMAX)[0]\n",
    "        prob_difference = MAX - subMAX\n",
    "        Prob_diff.append(prob_difference)\n",
    "        MAX_Prob_Mat.append(MAX)\n",
    "        subMAX_Prob_Mat.append(subMAX)\n",
    "        MAX_Prob_Mat_idx.append(MAX_idx)\n",
    "        subMAX_Prob_Mat_idx.append(subMAX_idx)\n",
    "    return Prob_diff,MAX_Prob_Mat,MAX_Prob_Mat_idx,subMAX_Prob_Mat,subMAX_Prob_Mat_idx\n",
    "def separate_a_number(num):\n",
    "    idx_train_num = find_idx(mnist_train_y,num)\n",
    "    idx_train_nonum = list(set(range(len(mnist_train_y))).difference(set(idx_train_num)))\n",
    "    mnist_trainx_nonum = mnist_train_x[idx_train_nonum]\n",
    "    mnist_trainx_num = mnist_train_x[idx_train_num]\n",
    "    mnist_trainy_nonum = mnist_train_y[idx_train_nonum]\n",
    "    mnist_trainy_num = mnist_train_y[idx_train_num]\n",
    "    idx_test_num = find_idx(mnist_test_y,num)\n",
    "    idx_test_nonum = list(set(range(len(mnist_test_y))).difference(set(idx_test_num)))\n",
    "    mnist_testx_nonum = mnist_test_x[idx_test_nonum]\n",
    "    mnist_testx_num = mnist_test_x[idx_test_num]\n",
    "    mnist_testy_nonum = mnist_test_y[idx_test_nonum]\n",
    "    mnist_testy_num = mnist_test_y[idx_test_num]\n",
    "\n",
    "    mnist_trainx_nonum, mnist_testx_nonum = mnist_trainx_nonum/255., mnist_testx_nonum/255.\n",
    "    mnist_trainx_num, mnist_testx_num = mnist_trainx_num/255., mnist_testx_num/255.\n",
    "    return mnist_trainx_nonum, mnist_trainy_nonum, mnist_testx_nonum, mnist_testy_nonum, mnist_trainx_num, mnist_trainy_num, mnist_testx_num, mnist_testy_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(mnist_train_x, mnist_train_y), (mnist_test_x, mnist_test_y)\\\n",
    "    = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(C_x_train, C_y_train), (C_x_test, C_y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "from skimage import exposure\n",
    "X_CIFAR_grey_re_train = []\n",
    "for i in range(len(C_x_train)):\n",
    "    A = 0.3*C_x_train[:,:,:,0][i]+\\\n",
    "           0.59*C_x_train[:,:,:,1][i]+\\\n",
    "           0.11*C_x_train[:,:,:,2][i]\n",
    "    A = exposure.rescale_intensity(A, out_range=(0, 255))\n",
    "    A = imutils.resize(A, width=28)\n",
    "    X_CIFAR_grey_re_train.append(A)\n",
    "X_CIFAR_grey_re_train = np.array(X_CIFAR_grey_re_train)\n",
    "\n",
    "X_CIFAR_grey_re_test = []\n",
    "for i in range(len(C_x_test)):\n",
    "    A = 0.3*C_x_test[:,:,:,0][i]+\\\n",
    "           0.59*C_x_test[:,:,:,1][i]+\\\n",
    "           0.11*C_x_test[:,:,:,2][i]\n",
    "    A = exposure.rescale_intensity(A, out_range=(0, 255))\n",
    "    A = imutils.resize(A, width=28)\n",
    "    X_CIFAR_grey_re_test.append(A)\n",
    "X_CIFAR_grey_re_test = np.array(X_CIFAR_grey_re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_the_orignal_index_after_ranking(arr, isReverse = True):\n",
    "    \"\"\"\n",
    "    This function return the orignal index of the after the array is sorted\n",
    "    inputs:\n",
    "    arr || one dimension list or ndarray\n",
    "    isReverse || boolean, if it is \"True\" the rank is decending; if it is \"False\" the rank is ascending \n",
    "    outputs:\n",
    "    A || an arr including the orignal index before ranking\n",
    "    ========================= examples =========================\n",
    "    For example, arr = [4,7,2,9]\n",
    "    we have the mapping relationship:\n",
    "    index    value\n",
    "      0        4\n",
    "      1        7\n",
    "      2        2\n",
    "      3        9\n",
    "    After sorting, say decendingly, we have:\n",
    "    orignal_index     value\n",
    "      3                 9\n",
    "      1                 7\n",
    "      0                 4\n",
    "      2                 2    \n",
    "    the result is for this function is [3,1,0,2].\n",
    "    \"\"\"\n",
    "    import operator\n",
    "    similarity_dict = dict(zip(list(range(len(arr))),arr))\n",
    "    sorted_similarity_dict = sorted(similarity_dict.items(), reverse=isReverse, key=operator.itemgetter(1))\n",
    "    A = [sorted_similarity_dict[i][0] for i in range(len(arr))]\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1025 14:57:56.332714 22456 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1025 14:57:56.347421 22456 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1025 14:57:56.421222 22456 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1025 14:57:56.434270 22456 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1025 14:57:56.747782 22456 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1025 14:57:56.751773 22456 deprecation_wrapper.py:119] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1025 14:57:56.809618 22456 deprecation.py:323] From c:\\users\\tiany\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2097 - acc: 0.9361\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0815 - acc: 0.9739\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0536 - acc: 0.9828\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0390 - acc: 0.9873\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0334 - acc: 0.9890\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0290 - acc: 0.9897\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0247 - acc: 0.9916\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0199 - acc: 0.9933\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0220 - acc: 0.9921\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0190 - acc: 0.9939\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "Training done, test accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "model = basic_mnist_model()\n",
    "training_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "          loss='sparse_categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.fit(mnist_train_x, mnist_train_y,\n",
    "      epochs=training_epochs,\n",
    "      batch_size=batch_size)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(mnist_test_x, mnist_test_y)\n",
    "print(\"Training done, test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = model.layers[7].output          # all layer outputs\n",
    "functors = K.function([inp, K.learning_phase()], [outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(x,y,n):\n",
    "    if np.isinf(n):\n",
    "        if n>0:\n",
    "            return np.max(np.abs(x-y))\n",
    "        else:\n",
    "            return np.min(np.abs(x-y))\n",
    "    else:\n",
    "        return np.power(np.sum(np.power(np.abs(x-y),n)),1/n)\n",
    "def minkowski_similarity(x,Y,n):\n",
    "    arr = []\n",
    "    for y in Y:\n",
    "        arr.append(minkowski_distance(x,y,n))\n",
    "    return np.array(arr)\n",
    "\n",
    "def get_KNN_stats(k,testarr_one_sample, testarr_waiting_to_compare, \n",
    "                  testarr_waiting_to_compare_label, Model, similarity_method = 'cosine_similarity', minkowski_power = 2):\n",
    "    \"\"\"\n",
    "    Inputs Example:\n",
    "    k = 50\n",
    "    testarr_one_sample = [mnist_train_RGB_x[0]]\n",
    "    testarr_waiting_to_compare = [C_x_train[i] for i in range(5000)]\n",
    "    testarr_waiting_to_compare_label = C_y_train[:5000].reshape(5000)\n",
    "    Model = C_VGG_Model1.model\n",
    "    \n",
    "    Inputs:\n",
    "    k: int, the number of the nearest neighbour\n",
    "    testarr_one_sample: multi-dimensional ndarray, shape = (1,num_pixel_x,num_pixel_y,num_channel)\n",
    "    testarr_waiting_to_compare_label: multi-dimensional ndarray, shape = (num_neighbour_candidate,num_pixel_x,num_pixel_y,num_channel)\n",
    "    testarr_waiting_to_compare_label: one-dimensional ndarray, shape = (num_neighbour_candidate,)\n",
    "    Model: keras backend model\n",
    "    similarity_method: String, 'cosine_similarity', 'minkowski_similarity'. Default = 'cosine_similarity'\n",
    "    minkowski_power: int, the p-value in the minkowski_distance. Only useful when similarity_method = 'minkowski_similarity'\n",
    "    \n",
    "    Outputs:\n",
    "    similarity: one-dimensional ndarray, shape = (num_neighbour_candidate,)\n",
    "    K_nearest_neighbour_orignal_label: one-dimensional ndarray, shape = (k,)\n",
    "    K_nearest_neighbour: multi-dimensional ndarray, shape = (k,num_pixel_x,num_pixel_y,num_channel)\n",
    "    KNN_oringal_class: dictionary, counts of the orignal class\n",
    "    max_ratio_KNN_from_one_class: float, the max of the ratio of KNN are from one class\n",
    "    \"\"\"\n",
    "    from keras import backend as K\n",
    "    inp = model.input                                           # input placeholder\n",
    "    outputs = model.layers[7].output          # all layer outputs\n",
    "    functors = K.function([inp, K.learning_phase()], [outputs])\n",
    "\n",
    "    # Testing\n",
    "    test1 = testarr_one_sample\n",
    "    layer_outs_one_sample = functors([test1, 0.])\n",
    "    layer_outs_one_sample = np.array(layer_outs_one_sample)[0]\n",
    "\n",
    "    test2 = testarr_waiting_to_compare\n",
    "    layer_outs_waiting_to_compare = functors([test2, 0.])\n",
    "    layer_outs_waiting_to_compare = np.array(layer_outs_waiting_to_compare)[0]\n",
    "    if similarity_method == 'cosine_similarity':\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarity = cosine_similarity(layer_outs_one_sample, layer_outs_waiting_to_compare)\n",
    "        similarity = np.array(similarity[0])\n",
    "    elif similarity_method == 'minkowski_similarity':\n",
    "        similarity = minkowski_similarity(layer_outs_one_sample, layer_outs_waiting_to_compare, minkowski_power)\n",
    "    else:\n",
    "        raise Exception(\"invalid similarity method\")\n",
    "    K_nearest_neighbour_orignal_label = get_the_orignal_index_after_ranking(similarity, isReverse = False)[:k]\n",
    "    K_nearest_neighbour_orignal_label = np.array(K_nearest_neighbour_orignal_label)\n",
    "    K_nearest_neighbour = testarr_waiting_to_compare[K_nearest_neighbour_orignal_label]\n",
    "    from collections import Counter\n",
    "    KNN_oringal_class = Counter(testarr_waiting_to_compare_label[K_nearest_neighbour_orignal_label])\n",
    "    max_ratio_KNN_from_one_class = max(KNN_oringal_class.values())/k\n",
    "    import operator\n",
    "    max_KNN_class_label = max(KNN_oringal_class.items(), key=operator.itemgetter(1))[0]\n",
    "    return similarity, K_nearest_neighbour_orignal_label, K_nearest_neighbour, KNN_oringal_class, max_ratio_KNN_from_one_class, max_KNN_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_representations(testarr_one_sample, testarr_waiting_to_compare,testarr_waiting_to_compare_label, MODEL, similarity_method = 'minkowski_similarity', minkowski_power = 2):\n",
    "    arr_KNN_from_same_class_ratio = []\n",
    "    arr_KNN_max_class_label = []\n",
    "    testarr_waiting_to_compare = testarr_waiting_to_compare[:5000]\n",
    "    testarr_waiting_to_compare_label = testarr_waiting_to_compare_label[:5000].reshape(5000)\n",
    "    for j in range(500):\n",
    "        print(\"current sample: \", j)\n",
    "        k = 50\n",
    "        testarr_one_sample_1 = [testarr_one_sample[j]]\n",
    "        result_l2 = get_KNN_stats(k,testarr_one_sample_1, testarr_waiting_to_compare, \n",
    "                  testarr_waiting_to_compare_label, MODEL, similarity_method = 'minkowski_similarity', minkowski_power = 2)\n",
    "        arr_KNN_from_same_class_ratio.append(result_l2[4])\n",
    "        arr_KNN_max_class_label.append(result_l2[5])\n",
    "\n",
    "    pred_labels = MODEL.predict(testarr_one_sample[:500])\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'arr_KNN_max_class_label':arr_KNN_max_class_label,\n",
    "                   'arr_KNN_from_same_class_ratio':arr_KNN_from_same_class_ratio,\n",
    "                   'predicted_label':find_statistics(pred_labels)[2],\n",
    "                  'predicted_prob':find_statistics(pred_labels)[1],})\n",
    "    return arr_KNN_from_same_class_ratio, arr_KNN_max_class_label, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sample:  0\n",
      "current sample:  1\n",
      "current sample:  2\n",
      "current sample:  3\n",
      "current sample:  4\n",
      "current sample:  5\n",
      "current sample:  6\n",
      "current sample:  7\n",
      "current sample:  8\n",
      "current sample:  9\n",
      "current sample:  10\n",
      "current sample:  11\n",
      "current sample:  12\n",
      "current sample:  13\n",
      "current sample:  14\n",
      "current sample:  15\n",
      "current sample:  16\n",
      "current sample:  17\n",
      "current sample:  18\n",
      "current sample:  19\n",
      "current sample:  20\n",
      "current sample:  21\n",
      "current sample:  22\n",
      "current sample:  23\n",
      "current sample:  24\n",
      "current sample:  25\n",
      "current sample:  26\n",
      "current sample:  27\n",
      "current sample:  28\n",
      "current sample:  29\n",
      "current sample:  30\n",
      "current sample:  31\n",
      "current sample:  32\n",
      "current sample:  33\n",
      "current sample:  34\n",
      "current sample:  35\n",
      "current sample:  36\n",
      "current sample:  37\n",
      "current sample:  38\n",
      "current sample:  39\n",
      "current sample:  40\n",
      "current sample:  41\n",
      "current sample:  42\n",
      "current sample:  43\n",
      "current sample:  44\n",
      "current sample:  45\n",
      "current sample:  46\n",
      "current sample:  47\n",
      "current sample:  48\n",
      "current sample:  49\n",
      "current sample:  50\n",
      "current sample:  51\n",
      "current sample:  52\n",
      "current sample:  53\n",
      "current sample:  54\n",
      "current sample:  55\n",
      "current sample:  56\n",
      "current sample:  57\n",
      "current sample:  58\n",
      "current sample:  59\n",
      "current sample:  60\n",
      "current sample:  61\n",
      "current sample:  62\n",
      "current sample:  63\n",
      "current sample:  64\n",
      "current sample:  65\n",
      "current sample:  66\n",
      "current sample:  67\n",
      "current sample:  68\n",
      "current sample:  69\n",
      "current sample:  70\n",
      "current sample:  71\n",
      "current sample:  72\n",
      "current sample:  73\n",
      "current sample:  74\n",
      "current sample:  75\n",
      "current sample:  76\n",
      "current sample:  77\n",
      "current sample:  78\n",
      "current sample:  79\n",
      "current sample:  80\n",
      "current sample:  81\n",
      "current sample:  82\n",
      "current sample:  83\n",
      "current sample:  84\n",
      "current sample:  85\n",
      "current sample:  86\n",
      "current sample:  87\n",
      "current sample:  88\n",
      "current sample:  89\n",
      "current sample:  90\n",
      "current sample:  91\n",
      "current sample:  92\n",
      "current sample:  93\n",
      "current sample:  94\n",
      "current sample:  95\n",
      "current sample:  96\n",
      "current sample:  97\n",
      "current sample:  98\n",
      "current sample:  99\n",
      "current sample:  100\n",
      "current sample:  101\n",
      "current sample:  102\n",
      "current sample:  103\n",
      "current sample:  104\n",
      "current sample:  105\n",
      "current sample:  106\n",
      "current sample:  107\n",
      "current sample:  108\n",
      "current sample:  109\n",
      "current sample:  110\n",
      "current sample:  111\n",
      "current sample:  112\n",
      "current sample:  113\n",
      "current sample:  114\n",
      "current sample:  115\n",
      "current sample:  116\n",
      "current sample:  117\n",
      "current sample:  118\n",
      "current sample:  119\n",
      "current sample:  120\n",
      "current sample:  121\n",
      "current sample:  122\n",
      "current sample:  123\n",
      "current sample:  124\n",
      "current sample:  125\n",
      "current sample:  126\n",
      "current sample:  127\n",
      "current sample:  128\n",
      "current sample:  129\n",
      "current sample:  130\n",
      "current sample:  131\n",
      "current sample:  132\n",
      "current sample:  133\n",
      "current sample:  134\n",
      "current sample:  135\n",
      "current sample:  136\n",
      "current sample:  137\n",
      "current sample:  138\n",
      "current sample:  139\n",
      "current sample:  140\n",
      "current sample:  141\n",
      "current sample:  142\n",
      "current sample:  143\n",
      "current sample:  144\n",
      "current sample:  145\n",
      "current sample:  146\n",
      "current sample:  147\n",
      "current sample:  148\n",
      "current sample:  149\n",
      "current sample:  150\n",
      "current sample:  151\n",
      "current sample:  152\n",
      "current sample:  153\n",
      "current sample:  154\n",
      "current sample:  155\n",
      "current sample:  156\n",
      "current sample:  157\n",
      "current sample:  158\n",
      "current sample:  159\n",
      "current sample:  160\n",
      "current sample:  161\n",
      "current sample:  162\n",
      "current sample:  163\n",
      "current sample:  164\n",
      "current sample:  165\n",
      "current sample:  166\n",
      "current sample:  167\n",
      "current sample:  168\n",
      "current sample:  169\n",
      "current sample:  170\n",
      "current sample:  171\n",
      "current sample:  172\n",
      "current sample:  173\n",
      "current sample:  174\n",
      "current sample:  175\n",
      "current sample:  176\n",
      "current sample:  177\n",
      "current sample:  178\n",
      "current sample:  179\n",
      "current sample:  180\n",
      "current sample:  181\n",
      "current sample:  182\n",
      "current sample:  183\n",
      "current sample:  184\n",
      "current sample:  185\n",
      "current sample:  186\n",
      "current sample:  187\n",
      "current sample:  188\n",
      "current sample:  189\n",
      "current sample:  190\n",
      "current sample:  191\n",
      "current sample:  192\n",
      "current sample:  193\n",
      "current sample:  194\n",
      "current sample:  195\n",
      "current sample:  196\n",
      "current sample:  197\n",
      "current sample:  198\n",
      "current sample:  199\n",
      "current sample:  200\n",
      "current sample:  201\n",
      "current sample:  202\n",
      "current sample:  203\n",
      "current sample:  204\n",
      "current sample:  205\n",
      "current sample:  206\n",
      "current sample:  207\n",
      "current sample:  208\n",
      "current sample:  209\n",
      "current sample:  210\n",
      "current sample:  211\n",
      "current sample:  212\n",
      "current sample:  213\n",
      "current sample:  214\n",
      "current sample:  215\n",
      "current sample:  216\n",
      "current sample:  217\n",
      "current sample:  218\n",
      "current sample:  219\n",
      "current sample:  220\n",
      "current sample:  221\n",
      "current sample:  222\n",
      "current sample:  223\n",
      "current sample:  224\n",
      "current sample:  225\n",
      "current sample:  226\n",
      "current sample:  227\n",
      "current sample:  228\n",
      "current sample:  229\n",
      "current sample:  230\n",
      "current sample:  231\n",
      "current sample:  232\n",
      "current sample:  233\n",
      "current sample:  234\n",
      "current sample:  235\n",
      "current sample:  236\n",
      "current sample:  237\n",
      "current sample:  238\n",
      "current sample:  239\n",
      "current sample:  240\n",
      "current sample:  241\n",
      "current sample:  242\n",
      "current sample:  243\n",
      "current sample:  244\n",
      "current sample:  245\n",
      "current sample:  246\n",
      "current sample:  247\n",
      "current sample:  248\n",
      "current sample:  249\n",
      "current sample:  250\n",
      "current sample:  251\n",
      "current sample:  252\n",
      "current sample:  253\n",
      "current sample:  254\n",
      "current sample:  255\n",
      "current sample:  256\n",
      "current sample:  257\n",
      "current sample:  258\n",
      "current sample:  259\n",
      "current sample:  260\n",
      "current sample:  261\n",
      "current sample:  262\n",
      "current sample:  263\n",
      "current sample:  264\n",
      "current sample:  265\n",
      "current sample:  266\n",
      "current sample:  267\n",
      "current sample:  268\n",
      "current sample:  269\n",
      "current sample:  270\n",
      "current sample:  271\n",
      "current sample:  272\n",
      "current sample:  273\n",
      "current sample:  274\n",
      "current sample:  275\n",
      "current sample:  276\n",
      "current sample:  277\n",
      "current sample:  278\n",
      "current sample:  279\n",
      "current sample:  280\n",
      "current sample:  281\n",
      "current sample:  282\n",
      "current sample:  283\n",
      "current sample:  284\n",
      "current sample:  285\n",
      "current sample:  286\n",
      "current sample:  287\n",
      "current sample:  288\n",
      "current sample:  289\n",
      "current sample:  290\n",
      "current sample:  291\n",
      "current sample:  292\n",
      "current sample:  293\n",
      "current sample:  294\n",
      "current sample:  295\n",
      "current sample:  296\n",
      "current sample:  297\n",
      "current sample:  298\n",
      "current sample:  299\n",
      "current sample:  300\n",
      "current sample:  301\n",
      "current sample:  302\n",
      "current sample:  303\n",
      "current sample:  304\n",
      "current sample:  305\n",
      "current sample:  306\n",
      "current sample:  307\n",
      "current sample:  308\n",
      "current sample:  309\n",
      "current sample:  310\n",
      "current sample:  311\n",
      "current sample:  312\n",
      "current sample:  313\n",
      "current sample:  314\n",
      "current sample:  315\n",
      "current sample:  316\n",
      "current sample:  317\n",
      "current sample:  318\n",
      "current sample:  319\n",
      "current sample:  320\n",
      "current sample:  321\n",
      "current sample:  322\n",
      "current sample:  323\n",
      "current sample:  324\n",
      "current sample:  325\n",
      "current sample:  326\n",
      "current sample:  327\n",
      "current sample:  328\n",
      "current sample:  329\n",
      "current sample:  330\n",
      "current sample:  331\n",
      "current sample:  332\n",
      "current sample:  333\n",
      "current sample:  334\n",
      "current sample:  335\n",
      "current sample:  336\n",
      "current sample:  337\n",
      "current sample:  338\n",
      "current sample:  339\n",
      "current sample:  340\n",
      "current sample:  341\n",
      "current sample:  342\n",
      "current sample:  343\n",
      "current sample:  344\n",
      "current sample:  345\n",
      "current sample:  346\n",
      "current sample:  347\n",
      "current sample:  348\n",
      "current sample:  349\n",
      "current sample:  350\n",
      "current sample:  351\n",
      "current sample:  352\n",
      "current sample:  353\n",
      "current sample:  354\n",
      "current sample:  355\n",
      "current sample:  356\n",
      "current sample:  357\n",
      "current sample:  358\n",
      "current sample:  359\n",
      "current sample:  360\n",
      "current sample:  361\n",
      "current sample:  362\n",
      "current sample:  363\n",
      "current sample:  364\n",
      "current sample:  365\n",
      "current sample:  366\n",
      "current sample:  367\n",
      "current sample:  368\n",
      "current sample:  369\n",
      "current sample:  370\n",
      "current sample:  371\n",
      "current sample:  372\n",
      "current sample:  373\n",
      "current sample:  374\n",
      "current sample:  375\n",
      "current sample:  376\n",
      "current sample:  377\n",
      "current sample:  378\n",
      "current sample:  379\n",
      "current sample:  380\n",
      "current sample:  381\n",
      "current sample:  382\n",
      "current sample:  383\n",
      "current sample:  384\n",
      "current sample:  385\n",
      "current sample:  386\n",
      "current sample:  387\n",
      "current sample:  388\n",
      "current sample:  389\n",
      "current sample:  390\n",
      "current sample:  391\n",
      "current sample:  392\n",
      "current sample:  393\n",
      "current sample:  394\n",
      "current sample:  395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sample:  396\n",
      "current sample:  397\n",
      "current sample:  398\n",
      "current sample:  399\n",
      "current sample:  400\n",
      "current sample:  401\n",
      "current sample:  402\n",
      "current sample:  403\n",
      "current sample:  404\n",
      "current sample:  405\n",
      "current sample:  406\n",
      "current sample:  407\n",
      "current sample:  408\n",
      "current sample:  409\n",
      "current sample:  410\n",
      "current sample:  411\n",
      "current sample:  412\n",
      "current sample:  413\n",
      "current sample:  414\n",
      "current sample:  415\n",
      "current sample:  416\n",
      "current sample:  417\n",
      "current sample:  418\n",
      "current sample:  419\n",
      "current sample:  420\n",
      "current sample:  421\n",
      "current sample:  422\n",
      "current sample:  423\n",
      "current sample:  424\n",
      "current sample:  425\n",
      "current sample:  426\n",
      "current sample:  427\n",
      "current sample:  428\n",
      "current sample:  429\n",
      "current sample:  430\n",
      "current sample:  431\n",
      "current sample:  432\n",
      "current sample:  433\n",
      "current sample:  434\n",
      "current sample:  435\n",
      "current sample:  436\n",
      "current sample:  437\n",
      "current sample:  438\n",
      "current sample:  439\n",
      "current sample:  440\n",
      "current sample:  441\n",
      "current sample:  442\n",
      "current sample:  443\n",
      "current sample:  444\n",
      "current sample:  445\n",
      "current sample:  446\n",
      "current sample:  447\n",
      "current sample:  448\n",
      "current sample:  449\n",
      "current sample:  450\n",
      "current sample:  451\n",
      "current sample:  452\n",
      "current sample:  453\n",
      "current sample:  454\n",
      "current sample:  455\n",
      "current sample:  456\n",
      "current sample:  457\n",
      "current sample:  458\n",
      "current sample:  459\n",
      "current sample:  460\n",
      "current sample:  461\n",
      "current sample:  462\n",
      "current sample:  463\n",
      "current sample:  464\n",
      "current sample:  465\n",
      "current sample:  466\n",
      "current sample:  467\n",
      "current sample:  468\n",
      "current sample:  469\n",
      "current sample:  470\n",
      "current sample:  471\n",
      "current sample:  472\n",
      "current sample:  473\n",
      "current sample:  474\n",
      "current sample:  475\n",
      "current sample:  476\n",
      "current sample:  477\n",
      "current sample:  478\n",
      "current sample:  479\n",
      "current sample:  480\n",
      "current sample:  481\n",
      "current sample:  482\n",
      "current sample:  483\n",
      "current sample:  484\n",
      "current sample:  485\n",
      "current sample:  486\n",
      "current sample:  487\n",
      "current sample:  488\n",
      "current sample:  489\n",
      "current sample:  490\n",
      "current sample:  491\n",
      "current sample:  492\n",
      "current sample:  493\n",
      "current sample:  494\n",
      "current sample:  495\n",
      "current sample:  496\n",
      "current sample:  497\n",
      "current sample:  498\n",
      "current sample:  499\n"
     ]
    }
   ],
   "source": [
    "result_compare = compare_representations(X_CIFAR_grey_re_train,mnist_test_x,mnist_test_y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arr_KNN_max_class_label</th>\n",
       "      <th>arr_KNN_from_same_class_ratio</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>7</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     arr_KNN_max_class_label  arr_KNN_from_same_class_ratio  predicted_label  \\\n",
       "0                          7                           0.44                1   \n",
       "1                          7                           0.44                1   \n",
       "2                          7                           0.46                1   \n",
       "3                          7                           0.42                1   \n",
       "4                          7                           0.44                1   \n",
       "5                          7                           0.44                1   \n",
       "6                          7                           0.44                1   \n",
       "7                          7                           0.42                1   \n",
       "8                          7                           0.44                1   \n",
       "9                          7                           0.44                1   \n",
       "10                         7                           0.44                1   \n",
       "11                         7                           0.44                1   \n",
       "12                         7                           0.44                1   \n",
       "13                         7                           0.46                1   \n",
       "14                         7                           0.44                1   \n",
       "15                         7                           0.44                1   \n",
       "16                         7                           0.46                1   \n",
       "17                         7                           0.44                1   \n",
       "18                         7                           0.46                1   \n",
       "19                         7                           0.44                1   \n",
       "20                         7                           0.44                1   \n",
       "21                         7                           0.44                1   \n",
       "22                         7                           0.44                1   \n",
       "23                         7                           0.44                1   \n",
       "24                         7                           0.44                1   \n",
       "25                         7                           0.44                1   \n",
       "26                         7                           0.44                1   \n",
       "27                         7                           0.44                1   \n",
       "28                         7                           0.44                1   \n",
       "29                         7                           0.44                1   \n",
       "..                       ...                            ...              ...   \n",
       "470                        7                           0.42                1   \n",
       "471                        7                           0.42                1   \n",
       "472                        7                           0.46                1   \n",
       "473                        7                           0.42                1   \n",
       "474                        7                           0.44                1   \n",
       "475                        7                           0.46                1   \n",
       "476                        7                           0.42                1   \n",
       "477                        7                           0.44                1   \n",
       "478                        7                           0.44                1   \n",
       "479                        7                           0.44                1   \n",
       "480                        7                           0.44                1   \n",
       "481                        7                           0.44                1   \n",
       "482                        7                           0.44                1   \n",
       "483                        7                           0.44                1   \n",
       "484                        7                           0.44                1   \n",
       "485                        7                           0.46                1   \n",
       "486                        7                           0.44                1   \n",
       "487                        7                           0.44                1   \n",
       "488                        7                           0.44                1   \n",
       "489                        7                           0.46                1   \n",
       "490                        7                           0.44                1   \n",
       "491                        7                           0.44                1   \n",
       "492                        7                           0.44                1   \n",
       "493                        7                           0.44                1   \n",
       "494                        7                           0.42                1   \n",
       "495                        7                           0.44                1   \n",
       "496                        7                           0.46                1   \n",
       "497                        7                           0.44                1   \n",
       "498                        7                           0.46                1   \n",
       "499                        7                           0.44                1   \n",
       "\n",
       "     predicted_prob  \n",
       "0               1.0  \n",
       "1               1.0  \n",
       "2               1.0  \n",
       "3               1.0  \n",
       "4               1.0  \n",
       "5               1.0  \n",
       "6               1.0  \n",
       "7               1.0  \n",
       "8               1.0  \n",
       "9               1.0  \n",
       "10              1.0  \n",
       "11              1.0  \n",
       "12              1.0  \n",
       "13              1.0  \n",
       "14              1.0  \n",
       "15              1.0  \n",
       "16              1.0  \n",
       "17              1.0  \n",
       "18              1.0  \n",
       "19              1.0  \n",
       "20              1.0  \n",
       "21              1.0  \n",
       "22              1.0  \n",
       "23              1.0  \n",
       "24              1.0  \n",
       "25              1.0  \n",
       "26              1.0  \n",
       "27              1.0  \n",
       "28              1.0  \n",
       "29              1.0  \n",
       "..              ...  \n",
       "470             1.0  \n",
       "471             1.0  \n",
       "472             1.0  \n",
       "473             1.0  \n",
       "474             1.0  \n",
       "475             1.0  \n",
       "476             1.0  \n",
       "477             1.0  \n",
       "478             1.0  \n",
       "479             1.0  \n",
       "480             1.0  \n",
       "481             1.0  \n",
       "482             1.0  \n",
       "483             1.0  \n",
       "484             1.0  \n",
       "485             1.0  \n",
       "486             1.0  \n",
       "487             1.0  \n",
       "488             1.0  \n",
       "489             1.0  \n",
       "490             1.0  \n",
       "491             1.0  \n",
       "492             1.0  \n",
       "493             1.0  \n",
       "494             1.0  \n",
       "495             1.0  \n",
       "496             1.0  \n",
       "497             1.0  \n",
       "498             1.0  \n",
       "499             1.0  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_compare[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = MODEL.predict(X_CIFAR_grey_re_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
